{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/fchollet/keras-resources\n",
    "\n",
    "# Dataset of 11,228 newswires from Reuters, labeled over 46 topics. \n",
    "# As with the IMDB dataset, each wire is encoded as a sequence of word indexes \n",
    "# (same conventions).\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "8982 train sequences\n",
      "2246 test sequences\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/datasets/#reuters-newswire-topics-classification\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
    "                                                         test_split=0.2)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ list([1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 2, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 2, 2, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 2, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12])\n",
      " list([1, 2, 699, 2, 2, 56, 2, 2, 9, 56, 2, 2, 81, 5, 2, 57, 366, 737, 132, 20, 2, 7, 2, 49, 2, 2, 2, 2, 699, 2, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2, 2, 2, 775, 7, 48, 34, 191, 44, 35, 2, 505, 17, 12])\n",
      " list([1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 2, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 2, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 2, 55, 2, 92, 617, 80, 2, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 2, 7, 2, 81, 5, 187, 11, 15, 9, 2, 201, 5, 47, 2, 18, 478, 2, 5, 2, 7, 232, 2, 71, 5, 160, 63, 11, 9, 2, 81, 5, 102, 59, 11, 17, 12])\n",
      " ...,\n",
      " list([1, 141, 2, 387, 81, 8, 16, 2, 10, 340, 2, 850, 31, 56, 2, 691, 9, 2, 71, 9, 2, 2, 2, 699, 2, 2, 2, 699, 244, 2, 4, 49, 8, 4, 656, 850, 33, 2, 9, 2, 340, 2, 2, 9, 2, 22, 2, 2, 687, 83, 35, 15, 257, 6, 57, 2, 7, 4, 2, 654, 5, 2, 2, 2, 4, 49, 8, 16, 369, 646, 6, 2, 7, 124, 407, 17, 12])\n",
      " list([1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, 258, 2, 18, 14, 74, 134, 2, 18, 88, 2, 72, 11, 14, 2, 32, 11, 123, 383, 89, 39, 46, 235, 10, 864, 728, 5, 258, 44, 11, 15, 22, 753, 9, 42, 92, 131, 728, 5, 69, 312, 11, 15, 22, 222, 2, 2, 383, 48, 39, 74, 235, 10, 864, 276, 5, 61, 32, 11, 15, 21, 4, 211, 5, 126, 2, 42, 92, 131, 46, 19, 352, 11, 15, 22, 710, 220, 9, 42, 92, 131, 276, 5, 59, 61, 11, 15, 22, 10, 455, 7, 2, 137, 336, 2, 6, 2, 142, 971, 2, 43, 359, 5, 4, 326, 753, 364, 17, 12])\n",
      " list([1, 227, 2, 91, 2, 125, 2, 21, 4, 2, 76, 7, 4, 757, 481, 2, 790, 2, 2, 9, 111, 149, 8, 7, 10, 76, 223, 51, 4, 417, 8, 2, 91, 2, 2, 340, 7, 194, 2, 6, 2, 21, 127, 2, 2, 2, 6, 2, 4, 329, 433, 7, 65, 87, 2, 10, 2, 2, 290, 9, 21, 567, 16, 2, 24, 4, 76, 209, 30, 2, 2, 2, 8, 4, 60, 8, 4, 966, 308, 40, 2, 129, 2, 295, 277, 2, 9, 24, 286, 2, 234, 222, 9, 4, 906, 2, 2, 114, 2, 2, 7, 4, 113, 17, 12])]\n"
     ]
    }
   ],
   "source": [
    "# look at data\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  4  3 ..., 25  3 25]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 classes\n"
     ]
    }
   ],
   "source": [
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing sequence data...\n",
      "x_train shape: (8982, 1000)\n",
      "x_test shape: (2246, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train_bin = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test_bin = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print('x_train shape:', x_train_bin.shape)\n",
    "print('x_test shape:', x_test_bin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  1. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# look at binary-encoded seq of word features\n",
    "print(x_train_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (8982, 46)\n",
      "y_test shape: (2246, 46)\n"
     ]
    }
   ],
   "source": [
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train_bin = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_bin = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train_bin.shape)\n",
    "print('y_test shape:', y_test_bin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))  # num features is the number of words\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/5\n",
      "8083/8083 [==============================] - 1s - loss: 1.4339 - acc: 0.6762 - val_loss: 1.0878 - val_acc: 0.7653\n",
      "Epoch 2/5\n",
      "8083/8083 [==============================] - 0s - loss: 0.7886 - acc: 0.8179 - val_loss: 0.9436 - val_acc: 0.7853\n",
      "Epoch 3/5\n",
      "8083/8083 [==============================] - 0s - loss: 0.5479 - acc: 0.8665 - val_loss: 0.8965 - val_acc: 0.7998\n",
      "Epoch 4/5\n",
      "8083/8083 [==============================] - 0s - loss: 0.4157 - acc: 0.8992 - val_loss: 0.8764 - val_acc: 0.8053\n",
      "Epoch 5/5\n",
      "8083/8083 [==============================] - 0s - loss: 0.3257 - acc: 0.9180 - val_loss: 0.9094 - val_acc: 0.7964\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_bin, y_train_bin,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984/2246 [=========================>....] - ETA: 0sTest score: 0.891361163115\n",
      "Test accuracy: 0.794746215494\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test_bin, y_test_bin,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Florida State quarterback Deondre Francois will miss the rest of the season after tearing the patellar tendon in his left knee in the fourth quarter Saturday night in a 24-7 loss to No. 1 Alabama in Atlanta.\n",
    "\n",
    "People familiar with the situation confirmed the injury to The Associated Press on Sunday night. They spoke on condition of anonymity because of the school’s medical information policy.\n",
    "\n",
    "The Tallahassee Democrat first reported the injury.\n",
    "\n",
    "Coach Jimbo Fisher is expected to have a further update, including when Francois will have surgery, during his weekly press conference Monday. Fisher said after Saturday’s game that if Francois was out, James Blackman would likely be the starter.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = keras.preprocessing.text.text_to_word_sequence(text,\n",
    "                                                        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                                        lower=True,\n",
    "                                                        split=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['florida', 'state', 'quarterback', 'deondre', 'francois', 'will', 'miss', 'the', 'rest', 'of']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = []\n",
    "for word in tokens:\n",
    "    try:\n",
    "        token_id = word_index[word]\n",
    "    except KeyError:\n",
    "        #token_id = 10000\n",
    "        pass\n",
    "    token_ids.append(token_id)\n",
    "token_ids = [token_ids]  # because Tokenizer.sequences_to_matrix expects list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2170, 275, 275, 275, 8026, 30, 6128, 1, 1350, 2, 1, 757, 89, 89, 1, 1, 1, 4, 268, 1206, 1206, 4, 1, 490, 95, 2368, 1402, 4, 7, 293, 58, 43, 3, 126, 16, 7160, 4, 4459, 869, 9275, 28, 1, 817, 1610, 1, 8088, 3, 1, 1706, 879, 18, 2377, 1402, 74, 5154, 18, 1857, 2, 7437, 158, 2, 1, 1, 1539, 986, 253, 1, 1, 2920, 96, 254, 1, 8088, 8088, 8088, 6035, 20, 130, 3, 54, 7, 223, 9432, 345, 182, 8026, 30, 54, 23426, 278, 268, 1210, 879, 519, 915, 6035, 5, 89, 89, 6574, 21, 104, 8026, 31, 239, 881, 26241, 38, 325, 27, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(token_ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (8982,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "token_ids_binary = tokenizer.sequences_to_matrix(token_ids, mode='binary')\n",
    "print('x_train shape:', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02750144,  0.01755367,  0.00333777,  0.11415914,  0.01237924,\n",
       "         0.00770436,  0.0058803 ,  0.01135649,  0.0469833 ,  0.01815013,\n",
       "         0.00366627,  0.05110939,  0.01279181,  0.01228714,  0.01462033,\n",
       "         0.01384593,  0.0128146 ,  0.00326411,  0.04919186,  0.05915083,\n",
       "         0.01599696,  0.02273646,  0.0048194 ,  0.03132347,  0.00684222,\n",
       "         0.01759172,  0.02818714,  0.01004125,  0.01873011,  0.01467679,\n",
       "         0.0140591 ,  0.03050522,  0.03104342,  0.00471372,  0.07257928,\n",
       "         0.01004172,  0.06032096,  0.01282194,  0.01943736,  0.00690318,\n",
       "         0.02826691,  0.00627373,  0.00598172,  0.02143936,  0.00355635,\n",
       "         0.00336254]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(token_ids_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(token_ids_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now look at other class 3's\n",
    "# x = numpy.array([1,0,2,0,3,0,4,5,6,7,8])\n",
    "# numpy.where(x == 0)[0]\n",
    "# array([1, 3, 5])\n",
    "\n",
    "cat_3_index = np.where(y_train == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_rev = {}\n",
    "for key, val in word_index.items():\n",
    "    word_index_rev[val] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_3_index = list(cat_3_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the of of mln loss for plc said at only ended said of could 1 traders now april 0 a after said from 1985 and from foreign 000 april 0 prices its account year a but in this mln home an states earlier and rise and revs vs 000 its 16 vs 000 a but 3 of of several and shareholders and dividend vs 000 its all 4 vs 000 1 mln agreed of april 0 are 2 states will billion total and against 000 pct dlrs\n",
      "\n",
      "the lt dlrs demand 000 reuter dividend year lt plus billion 04 000 reuter dividend year an worth new vs reuter dlrs of on shrs earnings countries new vs reuter 1985 billion vs 2 lt 4 division 000 reuter from go 000 lt plus which of 000 reuter from total 000 an 71 billion vs reuter dlr also vs shrs earnings countries 4 vs reuter 1985 from vs some now april 0 related in corp it inc strong cents dollar were after april 0 of or of more index 10 of company taking report it in estimated but trading texas said united said of a of up said countries vs 000 3 of central said which of on future of said of a includes of profit said meeting trade vs 3 of up said 1985 were vs pct dlrs\n",
      "\n",
      "the lt n year reuter trust year an of of reuter of of 1987 dlr also vs reuter owned had vs 2 lt from 71 end reuter from banking end an dlrs of of reuter billion of of 1987 before were vs reuter 18 which vs some but of estimated 3 2 an trading div company said of company's said countries of 000 3 of of 000 7 any year april 0 local exchange of estimated 3 2 an trading company a of most said of of 000 3 of of 000 7 because year april 0 local pct dlrs\n",
      "\n",
      "the major lt owned year reuter consumption year major an 4 of of reuter any were vs 1987 product trade vs reuter country's had vs some agreed of shareholders 4 vs 000 7 farm year lt reuter paid had vs 000 7 from standard 000 lt exchange an position point consumers lt company its then report tomorrow of 3 of figures said level one 3 conditions june of sources which vs treasury stocks had were is its for 2 reported pct dlrs\n",
      "\n",
      "the federal gain only growth lt they meeting year reuter company did year an they of of reuter company of of 1987 had of of reuter had profits of pct dlrs\n",
      "\n",
      "the 6 there of of added mln 1 he further american well a has would 86 of other delegates of its of from after 1 mln in support he 1986 will of of and 19 american of in this mln loss for plc said talks of stake said at sugar west or of a in compared analysts pct dlrs\n",
      "\n",
      "the use proposed addition year reuter account year 19 think prior revs ltd march 1985 pct dlrs\n",
      "\n",
      "the lt from did 000 reuter shareholders year lt plus from meeting 000 reuter shareholders year an meeting also vs reuter from of on 1987 of dlrs vs reuter worth from vs some but an trading home planned trade 1988 on 000 its gross 3 figures 3 of said funds of sugar 3 home 40 production trade vs 000 its united said of of company's producers dlrs signed on 000 said 40 its in of of united he will of a after january of of of of loan well pct dlrs\n",
      "\n",
      "the federal gain development foreign lt any year reuter revs year an billion billion vs reuter from dlrs vs may reserves were vs reuter point were vs pct dlrs\n",
      "\n",
      "the 2 and only growth into government europe 12 they billion ct s eight reuter dlrs quoted s into government france reserves 64 s reuter joint additional s into government due a computer sugar billion distribution s reuter billion worth s of this europe 12 they from of s reuter from of s of france sources addition s reuter production worth s of take france had revs s reuter trade countries s of japan france meeting standard s reuter all point s of due a computer sugar from countries s reuter of vs pct dlrs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for count, idx in enumerate(cat_3_index):\n",
    "    if count > 9:\n",
    "        break\n",
    "    seq_ids = x_train[idx]\n",
    "    class_3_txt = ' '.join([word_index_rev[_id] for _id in seq_ids])\n",
    "    print(class_3_txt)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these don't look like my input text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
